{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CSE4237 - Assignment 02 - P01.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W8me5UatlXN"
      },
      "source": [
        "P.S: USING LAB04 CODES TO GENERATE A BETTER MODEL ON MY PREVIOUS ASSIGNMENT WORKS\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zapTUNpYTeMW"
      },
      "source": [
        "# **BEST SETTINGS THAT I'VE FOUND IN MY WORK ~ 87% (86.99%)**\r\n",
        "\r\n",
        "*BATCH_SIZE* = 1621 \\\\\r\n",
        "AT *ITERATION* 14500 \\\\\r\n",
        "USING 5 *HIDDEN LAYERS* (each having 300 nodes, and *LeakyRELU* as activation layer)\r\n",
        "*Learining rate* was 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH_aXrd63y47"
      },
      "source": [
        "# **ASSIGNMENT 02**\r\n",
        "## **PROBLEM 1 :**\r\n",
        "#### **Apply only logistic regression for the  NumtaDB dataset and build a multiclass classification model that can recognize [0-9] Bengali handwritten digits with different hyperparameter settings.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LrpTvdo_4NiJ",
        "outputId": "6fa457af-63ed-4c34-c698-4e23d514bc1f"
      },
      "source": [
        "'''\r\n",
        "Name : Rahat Bin Osman\r\n",
        "ID: 160204083\r\n",
        "Group: B1\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nName : Rahat Bin Osman\\nID: 160204083\\nGroup: B1\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "z5qAWbqwlvgK",
        "outputId": "1b8c0eba-ac49-453d-fe39-be8436988cbd"
      },
      "source": [
        "!pip install kaggle\r\n",
        "from google.colab import files\r\n",
        "files.upload()\r\n",
        "!mkdir -p ~/.kaggle\r\n",
        "!cp kaggle.json ~/.kaggle/\r\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.10)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2020.12.5)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-caf9a0a1-3913-4451-a6ad-c7069f0caebd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-caf9a0a1-3913-4451-a6ad-c7069f0caebd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vI9ML-CxFrII",
        "outputId": "3f9b94ef-d901-45d8-c4cc-fe2fcbf49cc5"
      },
      "source": [
        "!kaggle datasets download -d BengaliAI/numta"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numta.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds3BCXInFvct"
      },
      "source": [
        "import os \r\n",
        "import zipfile \r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pandas as pd\r\n",
        "import shutil \r\n",
        "import matplotlib\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from PIL import Image\r\n",
        "from torch.utils.data import Dataset\r\n",
        "from torchvision import datasets, transforms, models\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import torchvision.datasets as dsets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EaJTpnQHLcm",
        "outputId": "682f2427-63b6-46bc-db70-426af91469dd"
      },
      "source": [
        "local_dir='/content/numta.zip'\r\n",
        "zip_ref=zipfile.ZipFile(local_dir,'r')\r\n",
        "zip_ref.extractall('/tmp')\r\n",
        "zip_ref.close"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method ZipFile.close of <zipfile.ZipFile filename='/content/numta.zip' mode='r'>>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKB8M9BVIuBE",
        "outputId": "240ced60-6153-48fb-985d-d20ae01ec4e8"
      },
      "source": [
        "PATH = '/tmp/'\r\n",
        "os.listdir(PATH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dap_multiplexer.68ca039041f2.root.log.INFO.20210306-183207.47',\n",
              " 'training-e',\n",
              " 'testing-d',\n",
              " 'training-a.csv',\n",
              " 'training-e.csv',\n",
              " 'testing-all-corrected',\n",
              " 'training-c',\n",
              " 'training-d.csv',\n",
              " 'training-d',\n",
              " 'testing-e',\n",
              " 'testing-auga',\n",
              " 'training-b.csv',\n",
              " 'train',\n",
              " 'dap_multiplexer.INFO',\n",
              " 'debugger_1rt0lxs13m',\n",
              " 'initgoogle_syslog_dir.0',\n",
              " 'testing-augc',\n",
              " 'testing-c',\n",
              " 'testing-f',\n",
              " 'testing-b',\n",
              " 'training-c.csv',\n",
              " 'training-b',\n",
              " 'testing-a',\n",
              " 'training-a']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKbePeqDHXPO"
      },
      "source": [
        "df=pd.read_csv(\"/tmp/training-a.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wy4GGyn4IL5Z",
        "outputId": "1a423fe7-8489-4ef8-8389-e88faa147640"
      },
      "source": [
        "df.columns.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['filename', 'original filename', 'scanid', 'digit',\n",
              "       'database name original', 'contributing team', 'database name'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0pqyLA5I_3d"
      },
      "source": [
        "col_list = [\"filename\", \"digit\"]\r\n",
        "\r\n",
        "df1=pd.read_csv(\"/tmp/training-a.csv\",usecols=col_list)\r\n",
        "df2=pd.read_csv(\"/tmp/training-b.csv\",usecols=col_list)\r\n",
        "df3=pd.read_csv(\"/tmp/training-c.csv\",usecols=col_list)\r\n",
        "df4=pd.read_csv(\"/tmp/training-d.csv\",usecols=col_list)\r\n",
        "df5=pd.read_csv(\"/tmp/training-e.csv\",usecols=col_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-df-wJ7J42n"
      },
      "source": [
        "dataframes=[df1,df2,df3,df4,df5]\r\n",
        "db = pd.concat(dataframes, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqxoknJKsTVR"
      },
      "source": [
        "**DATASET IS READY TO USE WITH A TOTAL OF 72045 IMAGES**\r\n",
        "\r\n",
        "> WHERE 90% will go for training and the rest 10% will be used for testing.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DIOwvQaLCgv",
        "outputId": "34acaffa-ef3d-4de5-f7bd-bbb57ec90585"
      },
      "source": [
        "db.info"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrame.info of          filename  digit\n",
              "0      a00000.png      5\n",
              "1      a00001.png      3\n",
              "2      a00002.png      1\n",
              "3      a00003.png      7\n",
              "4      a00004.png      0\n",
              "...           ...    ...\n",
              "72040  e16773.png      2\n",
              "72041  e16774.png      6\n",
              "72042  e16775.png      5\n",
              "72043  e16776.png      4\n",
              "72044  e16777.png      4\n",
              "\n",
              "[72045 rows x 2 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZsf1JJwNRsZ"
      },
      "source": [
        "if not os.path.exists(\"/tmp/train\"):\r\n",
        "    TRAIN_PATH=os.mkdir(\"/tmp/train\")\r\n",
        "\r\n",
        "ls1=os.listdir(\"/tmp/training-a\")\r\n",
        "ls2=os.listdir(\"/tmp/training-b\")\r\n",
        "ls3=os.listdir(\"/tmp/training-c\")\r\n",
        "ls4=os.listdir(\"/tmp/training-d\")\r\n",
        "ls5=os.listdir(\"/tmp/training-e\")\r\n",
        "\r\n",
        "src=\"/tmp/training-a\"\r\n",
        "TRAIN_PATH=\"/tmp/train\"\r\n",
        "for image in ls1:\r\n",
        "  file_name = os.path.join(src, image)\r\n",
        "  if os.path.isfile(file_name):\r\n",
        "   shutil.copy(file_name, TRAIN_PATH)  \r\n",
        "\r\n",
        "src=\"/tmp/training-b\"\r\n",
        "TRAIN_PATH=\"/tmp/train\"\r\n",
        "for image in ls2:\r\n",
        "  file_name = os.path.join(src, image)\r\n",
        "  if os.path.isfile(file_name):\r\n",
        "   shutil.copy(file_name, TRAIN_PATH)  \r\n",
        "\r\n",
        "src=\"/tmp/training-c\"\r\n",
        "TRAIN_PATH=\"/tmp/train\"\r\n",
        "for image in ls3:\r\n",
        "  file_name = os.path.join(src, image)\r\n",
        "  if os.path.isfile(file_name):\r\n",
        "   shutil.copy(file_name, TRAIN_PATH)  \r\n",
        "\r\n",
        "src=\"/tmp/training-d\"\r\n",
        "TRAIN_PATH=\"/tmp/train\"\r\n",
        "for image in ls4:\r\n",
        "  file_name = os.path.join(src, image)\r\n",
        "  if os.path.isfile(file_name):\r\n",
        "   shutil.copy(file_name, TRAIN_PATH)  \r\n",
        "\r\n",
        "src=\"/tmp/training-e\"\r\n",
        "TRAIN_PATH=\"/tmp/train\"\r\n",
        "for image in ls5:\r\n",
        "  file_name = os.path.join(src, image)\r\n",
        "  if os.path.isfile(file_name):\r\n",
        "   shutil.copy(file_name, TRAIN_PATH)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBsSF_ulO7aa",
        "outputId": "13904268-ac4f-4127-f724-95a89492be88"
      },
      "source": [
        "print(len(os.listdir(\"/tmp/train\")))\r\n",
        "print(len(db))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "72045\n",
            "72045\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCC76oGyPBTz"
      },
      "source": [
        "class Dataset(Dataset):\r\n",
        "    def __init__(self, df, root, transform=None):\r\n",
        "        self.data = df\r\n",
        "        self.root = root\r\n",
        "        self.transform = transform\r\n",
        "        \r\n",
        "    def __len__(self):\r\n",
        "        return len(self.data)\r\n",
        "    \r\n",
        "    def __getitem__(self, index):\r\n",
        "        item = self.data.iloc[index]\r\n",
        "        path = self.root + \"/\" + item[0]\r\n",
        "        image = Image.open(path).convert('L')\r\n",
        "        label = item[1]\r\n",
        "        \r\n",
        "        if self.transform is not None:\r\n",
        "            image = self.transform(image)\r\n",
        "        return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWb0DoydPNZp",
        "outputId": "1eee28b9-8dc2-490e-9b9d-14ba0bb417e4"
      },
      "source": [
        "mean = [0.5] \r\n",
        "std = [0.5]\r\n",
        "\r\n",
        "transform = transforms.Compose([\r\n",
        "    transforms.Resize((28,28)),\r\n",
        "    transforms.ToTensor(),\r\n",
        "    transforms.Normalize(mean, std)\r\n",
        "])\r\n",
        "\r\n",
        "train_data  = Dataset(db, TRAIN_PATH, transform)\r\n",
        "\r\n",
        "print(\"Training Samples:\",len(train_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Samples: 72045\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-iVVYl5Rd5K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9db12ca7-9788-4a94-bfce-09dd5a5a95be"
      },
      "source": [
        "train_dataset, test_dataset = train_test_split(train_data, test_size = 0.1)\r\n",
        "print(len(train_dataset))\r\n",
        "print(len(test_dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64840\n",
            "7205\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoLPb8Iya5JT"
      },
      "source": [
        "# **SETTING 01**\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0jY7KZ0E50C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db0bf728-0c96-4aab-f89e-fed730d3367e"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 100\n",
        "num_iters = 30000\n",
        "input_dim = 28*28 #num_features = 784\n",
        "num_hidden = 100\n",
        "output_dim = 10\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)  \n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.ReLU()\n",
        "\n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.ReLU()\n",
        "\n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "        \n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = num_hidden)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images) \n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 1.9079991579055786. Accuracy: 30.11797362942401\n",
            "Iteration: 1000. Loss: 1.805791974067688. Accuracy: 31.43650242886884\n",
            "Iteration: 1500. Loss: 2.032890796661377. Accuracy: 29.22970159611381\n",
            "Iteration: 2000. Loss: 1.8418611288070679. Accuracy: 31.658570437196392\n",
            "Iteration: 2500. Loss: 1.9723045825958252. Accuracy: 33.96252602359473\n",
            "Iteration: 3000. Loss: 1.732933521270752. Accuracy: 42.664816099930604\n",
            "Iteration: 3500. Loss: 1.7209173440933228. Accuracy: 39.19500346981263\n",
            "Iteration: 4000. Loss: 1.5846437215805054. Accuracy: 43.78903539208883\n",
            "Iteration: 4500. Loss: 1.629274845123291. Accuracy: 49.673837612768914\n",
            "Iteration: 5000. Loss: 1.5521982908248901. Accuracy: 40.360860513532266\n",
            "Iteration: 5500. Loss: 1.4526848793029785. Accuracy: 46.05135322692575\n",
            "Iteration: 6000. Loss: 1.457910180091858. Accuracy: 46.67591950034698\n",
            "Iteration: 6500. Loss: 1.349843144416809. Accuracy: 46.16238723108952\n",
            "Iteration: 7000. Loss: 1.3347256183624268. Accuracy: 55.475364330326165\n",
            "Iteration: 7500. Loss: 1.3866667747497559. Accuracy: 53.03261623872311\n",
            "Iteration: 8000. Loss: 1.3114889860153198. Accuracy: 55.544760582928525\n",
            "Iteration: 8500. Loss: 1.1115306615829468. Accuracy: 57.90423317140874\n",
            "Iteration: 9000. Loss: 1.3089884519577026. Accuracy: 55.43372657876475\n",
            "Iteration: 9500. Loss: 0.9023409485816956. Accuracy: 65.02428868841082\n",
            "Iteration: 10000. Loss: 1.0289417505264282. Accuracy: 65.35739070090216\n",
            "Iteration: 10500. Loss: 1.0992207527160645. Accuracy: 55.50312283136711\n",
            "Iteration: 11000. Loss: 0.9685755372047424. Accuracy: 63.761276891047885\n",
            "Iteration: 11500. Loss: 0.9419143795967102. Accuracy: 65.26023594725885\n",
            "Iteration: 12000. Loss: 1.1061445474624634. Accuracy: 61.99861207494795\n",
            "Iteration: 12500. Loss: 1.1152260303497314. Accuracy: 66.35669673837613\n",
            "Iteration: 13000. Loss: 0.8042542934417725. Accuracy: 70.49271339347675\n",
            "Iteration: 13500. Loss: 0.9231602549552917. Accuracy: 70.72866065232478\n",
            "Iteration: 14000. Loss: 0.9441651701927185. Accuracy: 70.58986814712006\n",
            "Iteration: 14500. Loss: 0.7779729962348938. Accuracy: 70.75641915336571\n",
            "Iteration: 15000. Loss: 0.8540736436843872. Accuracy: 65.9264399722415\n",
            "Iteration: 15500. Loss: 0.7897434234619141. Accuracy: 70.85357390700902\n",
            "Iteration: 16000. Loss: 0.8689776062965393. Accuracy: 72.65787647467037\n",
            "Iteration: 16500. Loss: 1.0084530115127563. Accuracy: 71.86675919500347\n",
            "Iteration: 17000. Loss: 0.683423638343811. Accuracy: 72.81054823039555\n",
            "Iteration: 17500. Loss: 1.0233381986618042. Accuracy: 64.38584316446912\n",
            "Iteration: 18000. Loss: 0.6244513392448425. Accuracy: 76.1970853573907\n",
            "Iteration: 18500. Loss: 0.5813970565795898. Accuracy: 74.62873004857738\n",
            "Iteration: 19000. Loss: 0.6265421509742737. Accuracy: 72.82442748091603\n",
            "Iteration: 19500. Loss: 0.5993282794952393. Accuracy: 74.18459403192227\n",
            "Iteration: 20000. Loss: 0.7405952215194702. Accuracy: 76.26648160999306\n",
            "Iteration: 20500. Loss: 0.6730064153671265. Accuracy: 72.67175572519083\n",
            "Iteration: 21000. Loss: 0.8409706354141235. Accuracy: 75.47536433032616\n",
            "Iteration: 21500. Loss: 0.6440370082855225. Accuracy: 78.98681471200555\n",
            "Iteration: 22000. Loss: 0.6907879114151001. Accuracy: 74.62873004857738\n",
            "Iteration: 22500. Loss: 0.665678858757019. Accuracy: 76.11380985426787\n",
            "Iteration: 23000. Loss: 0.7010412812232971. Accuracy: 75.76682859125607\n",
            "Iteration: 23500. Loss: 0.6481998562812805. Accuracy: 74.68424705065927\n",
            "Iteration: 24000. Loss: 0.5078644752502441. Accuracy: 80.05551700208188\n",
            "Iteration: 24500. Loss: 0.6261323690414429. Accuracy: 76.01665510062456\n",
            "Iteration: 25000. Loss: 0.5262953639030457. Accuracy: 76.72449687716863\n",
            "Iteration: 25500. Loss: 0.4004351794719696. Accuracy: 80.8882720333102\n",
            "Iteration: 26000. Loss: 0.5614451766014099. Accuracy: 79.52810548230396\n",
            "Iteration: 26500. Loss: 0.4135785698890686. Accuracy: 76.94656488549619\n",
            "Iteration: 27000. Loss: 0.3821220099925995. Accuracy: 80.63844552394171\n",
            "Iteration: 27500. Loss: 0.6962058544158936. Accuracy: 71.71408743927827\n",
            "Iteration: 28000. Loss: 0.5490105152130127. Accuracy: 80.86051353226925\n",
            "Iteration: 28500. Loss: 0.5942860841751099. Accuracy: 77.15475364330327\n",
            "Iteration: 29000. Loss: 0.6461499929428101. Accuracy: 80.20818875780708\n",
            "Iteration: 29500. Loss: 0.5864048600196838. Accuracy: 80.31922276197085\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoP8uowAT8j3"
      },
      "source": [
        "# **SETTING 02**\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UmabeN4QECw",
        "outputId": "954266ec-ff4b-43ee-9d5e-0339e6e66127"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 1621\n",
        "num_iters = 12000\n",
        "input_dim = 28*28 #num_features = 784\n",
        "num_hidden = 100\n",
        "output_dim = 10\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)  \n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.LeakyReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.LeakyReLU()\n",
        "\n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.LeakyReLU()\n",
        "\n",
        "        ### 4th hidden layer: 100 --> 100\n",
        "        self.linear_4 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_4 = nn.LeakyReLU()\n",
        "\n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "\n",
        "         ### 4th hidden layer\n",
        "        out  = self.linear_4(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_4(out)\n",
        "        \n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = num_hidden)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images) \n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 2.0142323970794678. Accuracy: 23.913948646773076\n",
            "Iteration: 1000. Loss: 1.8226474523544312. Accuracy: 30.97848716169327\n",
            "Iteration: 1500. Loss: 1.7255643606185913. Accuracy: 33.088133240805\n",
            "Iteration: 2000. Loss: 1.7743223905563354. Accuracy: 38.6675919500347\n",
            "Iteration: 2500. Loss: 1.654906988143921. Accuracy: 46.717557251908396\n",
            "Iteration: 3000. Loss: 1.6134486198425293. Accuracy: 37.65440666204025\n",
            "Iteration: 3500. Loss: 1.6101983785629272. Accuracy: 45.53782095766829\n",
            "Iteration: 4000. Loss: 1.4461123943328857. Accuracy: 51.33934767522554\n",
            "Iteration: 4500. Loss: 1.606313705444336. Accuracy: 43.59472588480222\n",
            "Iteration: 5000. Loss: 1.261786699295044. Accuracy: 50.978487161693266\n",
            "Iteration: 5500. Loss: 1.3908493518829346. Accuracy: 51.36710617626648\n",
            "Iteration: 6000. Loss: 1.2241040468215942. Accuracy: 47.369882026370576\n",
            "Iteration: 6500. Loss: 1.3232873678207397. Accuracy: 59.916724496877166\n",
            "Iteration: 7000. Loss: 1.1154199838638306. Accuracy: 49.49340735600278\n",
            "Iteration: 7500. Loss: 1.1703479290008545. Accuracy: 60.388619014573216\n",
            "Iteration: 8000. Loss: 0.9550809860229492. Accuracy: 62.13740458015267\n",
            "Iteration: 8500. Loss: 1.003109097480774. Accuracy: 67.74462179042332\n",
            "Iteration: 9000. Loss: 0.8658709526062012. Accuracy: 70.89521165857043\n",
            "Iteration: 9500. Loss: 0.8457028269767761. Accuracy: 65.10756419153365\n",
            "Iteration: 10000. Loss: 0.826161801815033. Accuracy: 70.28452463566967\n",
            "Iteration: 10500. Loss: 0.6913506984710693. Accuracy: 69.78487161693269\n",
            "Iteration: 11000. Loss: 0.6794179677963257. Accuracy: 71.89451769604442\n",
            "Iteration: 11500. Loss: 0.5870423913002014. Accuracy: 77.83483691880639\n",
            "Iteration: 12000. Loss: 0.5116345286369324. Accuracy: 77.75156141568355\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cNUu-_KT5IB"
      },
      "source": [
        "# **SETTING 03**\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78fO7dRsTIGQ",
        "outputId": "4d36ad10-d0a5-434b-e6d6-49ec266f0d24"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 1621\n",
        "num_iters = 12000\n",
        "input_dim = 28*28 #num_features = 784\n",
        "num_hidden = 300\n",
        "output_dim = 10\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)  \n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.LeakyReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.LeakyReLU()\n",
        "\n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.LeakyReLU()\n",
        "\n",
        "        ### 4th hidden layer: 100 --> 100\n",
        "        self.linear_4 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_4 = nn.LeakyReLU()\n",
        "\n",
        "        ### 5th hidden layer: 100 --> 100\n",
        "        self.linear_5 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_5 = nn.LeakyReLU()\n",
        "\n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "\n",
        "         ### 4th hidden layer\n",
        "        out  = self.linear_4(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_4(out)\n",
        "\n",
        "        ### 5th hidden layer\n",
        "        out  = self.linear_5(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_5(out)\n",
        "        \n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = num_hidden)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images) \n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 2.279655694961548. Accuracy: 15.794587092297016\n",
            "Iteration: 1000. Loss: 1.9512073993682861. Accuracy: 28.57737682165163\n",
            "Iteration: 1500. Loss: 1.783829927444458. Accuracy: 34.823039555863986\n",
            "Iteration: 2000. Loss: 1.6334381103515625. Accuracy: 36.28036086051353\n",
            "Iteration: 2500. Loss: 1.6513386964797974. Accuracy: 31.700208188757806\n",
            "Iteration: 3000. Loss: 1.6295095682144165. Accuracy: 38.986814712005554\n",
            "Iteration: 3500. Loss: 1.5966248512268066. Accuracy: 38.00138792505205\n",
            "Iteration: 4000. Loss: 1.4578416347503662. Accuracy: 51.117279666897986\n",
            "Iteration: 4500. Loss: 1.395443081855774. Accuracy: 49.09090909090909\n",
            "Iteration: 5000. Loss: 1.5464473962783813. Accuracy: 45.05204718945177\n",
            "Iteration: 5500. Loss: 1.729675054550171. Accuracy: 45.45454545454545\n",
            "Iteration: 6000. Loss: 1.3096617460250854. Accuracy: 54.54545454545455\n",
            "Iteration: 6500. Loss: 1.2352452278137207. Accuracy: 58.14018043025676\n",
            "Iteration: 7000. Loss: 1.2660412788391113. Accuracy: 50.298403886190144\n",
            "Iteration: 7500. Loss: 1.946631908416748. Accuracy: 43.99722414989591\n",
            "Iteration: 8000. Loss: 1.117294192314148. Accuracy: 54.01804302567661\n",
            "Iteration: 8500. Loss: 0.8002892136573792. Accuracy: 67.8972935461485\n",
            "Iteration: 9000. Loss: 0.9404205679893494. Accuracy: 72.2970159611381\n",
            "Iteration: 9500. Loss: 0.6140341758728027. Accuracy: 75.26717557251908\n",
            "Iteration: 10000. Loss: 0.6908305883407593. Accuracy: 75.47536433032616\n",
            "Iteration: 10500. Loss: 0.6129826307296753. Accuracy: 73.69882026370576\n",
            "Iteration: 11000. Loss: 0.5133964419364929. Accuracy: 79.81956974323387\n",
            "Iteration: 11500. Loss: 0.6025970578193665. Accuracy: 82.09576682859125\n",
            "Iteration: 12000. Loss: 0.4285041093826294. Accuracy: 83.74739764052741\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSH3C3EyT2Gm"
      },
      "source": [
        "# **SETTING 04**\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvBjCgylW7f0"
      },
      "source": [
        "# **BEST SETTINGS THAT I'VE FOUND IN MY WORK ~ 87% (86.99%)**\r\n",
        "\r\n",
        "*BATCH_SIZE* = 1621 \\\\\r\n",
        "AT *ITERATION* 14500 \\\\\r\n",
        "USING 5 *HIDDEN LAYERS* (each having 300 nodes, and *LeakyRELU* as activation layer)\r\n",
        "*Learining rate* was 0.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYv0t2w_TyQ4",
        "outputId": "e20134e3-5c02-46a5-a449-e92aa4e31d4d"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 1621\n",
        "num_iters = 15000\n",
        "input_dim = 28*28 #num_features = 784\n",
        "num_hidden = 300\n",
        "output_dim = 10\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)  \n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.LeakyReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.LeakyReLU()\n",
        "\n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.LeakyReLU()\n",
        "\n",
        "        ### 4th hidden layer: 100 --> 100\n",
        "        self.linear_4 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_4 = nn.LeakyReLU()\n",
        "\n",
        "        ### 5th hidden layer: 100 --> 100\n",
        "        self.linear_5 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_5 = nn.LeakyReLU()\n",
        "\n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "\n",
        "         ### 4th hidden layer\n",
        "        out  = self.linear_4(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_4(out)\n",
        "\n",
        "        ### 5th hidden layer\n",
        "        out  = self.linear_5(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_5(out)\n",
        "        \n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = num_hidden)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images) \n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 2.2745096683502197. Accuracy: 21.17973629424011\n",
            "Iteration: 1000. Loss: 1.8771384954452515. Accuracy: 27.300485773768216\n",
            "Iteration: 1500. Loss: 1.8066753149032593. Accuracy: 27.605829285218597\n",
            "Iteration: 2000. Loss: 1.6924033164978027. Accuracy: 40.41637751561416\n",
            "Iteration: 2500. Loss: 1.7915600538253784. Accuracy: 34.26786953504511\n",
            "Iteration: 3000. Loss: 1.5187245607376099. Accuracy: 44.066620402498266\n",
            "Iteration: 3500. Loss: 1.5696516036987305. Accuracy: 50.38167938931298\n",
            "Iteration: 4000. Loss: 1.4078588485717773. Accuracy: 48.2442748091603\n",
            "Iteration: 4500. Loss: 1.329304575920105. Accuracy: 50.58986814712006\n",
            "Iteration: 5000. Loss: 1.4561400413513184. Accuracy: 50.367800138792504\n",
            "Iteration: 5500. Loss: 1.4135366678237915. Accuracy: 46.95350451075642\n",
            "Iteration: 6000. Loss: 1.3669509887695312. Accuracy: 59.083969465648856\n",
            "Iteration: 6500. Loss: 0.9928193688392639. Accuracy: 58.94517696044414\n",
            "Iteration: 7000. Loss: 1.4341951608657837. Accuracy: 46.12074947952811\n",
            "Iteration: 7500. Loss: 1.0582088232040405. Accuracy: 64.12213740458016\n",
            "Iteration: 8000. Loss: 0.8814502954483032. Accuracy: 63.95558639833449\n",
            "Iteration: 8500. Loss: 0.7155064940452576. Accuracy: 70.14573213046495\n",
            "Iteration: 9000. Loss: 1.0653003454208374. Accuracy: 56.446911866759194\n",
            "Iteration: 9500. Loss: 0.6175575852394104. Accuracy: 68.4941013185288\n",
            "Iteration: 10000. Loss: 0.9324227571487427. Accuracy: 60.68008327550312\n",
            "Iteration: 10500. Loss: 0.516192615032196. Accuracy: 82.1235253296322\n",
            "Iteration: 11000. Loss: 0.4908275306224823. Accuracy: 80.99930603747397\n",
            "Iteration: 11500. Loss: 0.45336636900901794. Accuracy: 81.31852879944483\n",
            "Iteration: 12000. Loss: 0.37136346101760864. Accuracy: 82.02637057598889\n",
            "Iteration: 12500. Loss: 0.44906526803970337. Accuracy: 77.41845940319223\n",
            "Iteration: 13000. Loss: 0.35547715425491333. Accuracy: 84.76058292852186\n",
            "Iteration: 13500. Loss: 0.31090283393859863. Accuracy: 84.64954892435809\n",
            "Iteration: 14000. Loss: 0.2917937636375427. Accuracy: 86.99514226231784\n",
            "Iteration: 14500. Loss: 0.515198826789856. Accuracy: 84.2470506592644\n",
            "Iteration: 15000. Loss: 0.8624370694160461. Accuracy: 73.08813324080499\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiaNQCKeoxUu"
      },
      "source": [
        "# **PREDICTIONS IN TEST DATAS**\r\n",
        "### **(45% ACCURACY MODEL)**\r\n",
        "- We used the 5th model that had the following settings:\r\n",
        "  - batch_size = 1621 \r\n",
        "  - num_iters = 30000\r\n",
        "  - num_features = 784\r\n",
        "  - output_dim = 10\r\n",
        "  - learning_rate = 0.003\r\n",
        "\r\n"
      ]
    }
  ]
}