{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSE4237 - Assignment 02 - P02.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W8me5UatlXN"
      },
      "source": [
        "P.S: USING LAB04 CODES TO GENERATE A BETTER MODEL ON MY PREVIOUS ASSIGNMENT WORKS\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0QBEHTRRpPn"
      },
      "source": [
        "# **BEST SETTINGS THAT I'VE FOUND IN MY WORK = 62.24%**\r\n",
        "\r\n",
        "*BATCH_SIZE* = 200 \\\\\r\n",
        "AT *ITERATION* 13500 \\\\\r\n",
        "USING 5 *HIDDEN LAYERS* (each having 300 nodes, and *LeakyRELU* as activation layer)\r\n",
        "*Learining rate* was 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyqFNMF3ZCC5"
      },
      "source": [
        "# **ASSIGNMENT 02**\r\n",
        "## **PROBLEM 02 :**\r\n",
        "#### Apply only **logistic regression** for the **Ekush** dataset and build a binary classification model that can predict  **male/female** from Bengali handwritten digits with different hyperparameter settings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyK6xI1DZj2l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fbb7a9a8-54e1-42f8-9ade-3127963da468"
      },
      "source": [
        "'''\r\n",
        "Name : Rahat Bin Osman\r\n",
        "ID: 160204083\r\n",
        "Group: B1\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nName : Rahat Bin Osman\\nID: 160204083\\nGroup: B1\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqG8CNXs7mPX",
        "outputId": "ba01ca87-49e9-46e6-a93d-905bd2068777"
      },
      "source": [
        "!gdown --id 1eVuR7pbjFLDmI8xL5qybrSau27vNNeSo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1eVuR7pbjFLDmI8xL5qybrSau27vNNeSo\n",
            "To: /content/maleDigits.csv\n",
            "30.8MB [00:00, 144MB/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_wKiH1_--q9",
        "outputId": "436018eb-a5f2-4905-b8f9-43c696b9d643"
      },
      "source": [
        "!gdown --id 1BuUNNQkR8T6ALoRvsTY-mApWFvts27BC"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1BuUNNQkR8T6ALoRvsTY-mApWFvts27BC\n",
            "To: /content/femaleDigits.csv\n",
            "32.1MB [00:00, 122MB/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEG6NHcD_dtR"
      },
      "source": [
        "import os \r\n",
        "import zipfile \r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pandas as pd\r\n",
        "import shutil \r\n",
        "import matplotlib\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from PIL import Image\r\n",
        "from torch.utils.data import Dataset\r\n",
        "from torchvision import datasets, transforms, models\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import torchvision.datasets as dsets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vmCvu-4_uBN"
      },
      "source": [
        "df1=pd.read_csv(\"/content/maleDigits.csv\")\r\n",
        "df2=pd.read_csv(\"/content/femaleDigits.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwI_70pj_-UX"
      },
      "source": [
        "'''\r\n",
        "Let label 0 for male\r\n",
        "and \r\n",
        "Let label 1 for female\r\n",
        "'''\r\n",
        "\r\n",
        "df1['label'] = 0\r\n",
        "df2['label'] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQ84N9zbBFU3",
        "outputId": "f44ef322-6e91-4843-b121-3dae1a3bb53b"
      },
      "source": [
        "print (df1['label'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0        0\n",
            "1        0\n",
            "2        0\n",
            "3        0\n",
            "4        0\n",
            "        ..\n",
            "15203    0\n",
            "15204    0\n",
            "15205    0\n",
            "15206    0\n",
            "15207    0\n",
            "Name: label, Length: 15208, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcmwBhOBBTmQ",
        "outputId": "64887e4c-81df-418b-949d-bd83bcb1b90c"
      },
      "source": [
        "dataframes=[df1,df2]\r\n",
        "db_all = pd.concat(dataframes, ignore_index=True)\r\n",
        "print(db_all.shape)\r\n",
        "#db_all['label']\r\n",
        "db_labels=db_all['label']\r\n",
        "print(db_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30830, 785)\n",
            "0        0\n",
            "1        0\n",
            "2        0\n",
            "3        0\n",
            "4        0\n",
            "        ..\n",
            "30825    1\n",
            "30826    1\n",
            "30827    1\n",
            "30828    1\n",
            "30829    1\n",
            "Name: label, Length: 30830, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xz3cgOuVDFA6",
        "outputId": "1a6b4b7f-2a42-4d16-e3e8-0bf95eea4e3d"
      },
      "source": [
        "db_img = db_all.drop(labels = [\"label\"],axis = 1)\r\n",
        "print(db_img.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30830, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzYUFO9DvP3o"
      },
      "source": [
        "**DATASET IS READY TO USE WITH A TOTAL OF 30830 IMAGES**\r\n",
        "\r\n",
        "> WHERE 90% will go for training and the rest 10% will be used for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVdTWy7LCW-V"
      },
      "source": [
        "db_img = db_img/255.0 #normalize\r\n",
        "db_img = db_img.values.reshape(-1,28,28,1) #Reshape the array into 28 x 28 pixel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "YfY7ZzUzDttw",
        "outputId": "6d270fa3-3937-4107-92cd-d5a57e47e95b"
      },
      "source": [
        "'''\r\n",
        "plotting the img \r\n",
        "'''\r\n",
        "show_img = db_img[30800].reshape(28,28)\r\n",
        "plt.imshow(show_img, cmap='gray')\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa9c1800a50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwwLM3-dEZDC"
      },
      "source": [
        "train_img, test_img, train_label, test_label = train_test_split(db_img, db_labels, test_size = 0.1)\r\n",
        "train_dataset = list(zip(train_img,train_label))\r\n",
        "test_dataset = list(zip(test_img,test_label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ry1iZRE2GoVM",
        "outputId": "6a42c2d9-87e5-4408-b4d2-0ab68fc0013e"
      },
      "source": [
        "print(len(train_dataset))\r\n",
        "print(len(test_dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27747\n",
            "3083\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoLPb8Iya5JT"
      },
      "source": [
        "# **SETTING 01**\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0jY7KZ0E50C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76e84420-1c94-49f5-cd24-70c898941332"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 100\n",
        "num_iters = 30000\n",
        "input_dim = 28*28 #num_features = 784\n",
        "num_hidden = 1000\n",
        "output_dim = 10\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)  \n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False) \n",
        "\n",
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.ReLU()\n",
        "\n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.ReLU()\n",
        "\n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x.float())\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "        \n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = num_hidden)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images.float()) \n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 0.6989140510559082. Accuracy: 60.88225754135582\n",
            "Iteration: 1000. Loss: 0.655036211013794. Accuracy: 62.082387285111906\n",
            "Iteration: 1500. Loss: 0.6511877179145813. Accuracy: 61.758027894907556\n",
            "Iteration: 2000. Loss: 0.637754499912262. Accuracy: 61.88777165098929\n",
            "Iteration: 2500. Loss: 0.520005464553833. Accuracy: 61.62828413882582\n",
            "Iteration: 3000. Loss: 0.6503285765647888. Accuracy: 59.584819980538434\n",
            "Iteration: 3500. Loss: 0.516806423664093. Accuracy: 62.47161855335712\n",
            "Iteration: 4000. Loss: 0.5784268379211426. Accuracy: 59.26046059033409\n",
            "Iteration: 4500. Loss: 0.47842350602149963. Accuracy: 60.81738566331495\n",
            "Iteration: 5000. Loss: 0.5373415946960449. Accuracy: 57.703535517353224\n",
            "Iteration: 5500. Loss: 0.5592882633209229. Accuracy: 59.22802465131365\n",
            "Iteration: 6000. Loss: 0.257182776927948. Accuracy: 61.30392474862147\n",
            "Iteration: 6500. Loss: 0.25111672282218933. Accuracy: 60.46059033409017\n",
            "Iteration: 7000. Loss: 0.22153052687644958. Accuracy: 59.97405124878365\n",
            "Iteration: 7500. Loss: 0.24543972313404083. Accuracy: 59.6496918585793\n",
            "Iteration: 8000. Loss: 0.2487969547510147. Accuracy: 56.98994485890366\n",
            "Iteration: 8500. Loss: 0.14383313059806824. Accuracy: 59.26046059033409\n",
            "Iteration: 9000. Loss: 0.1396186649799347. Accuracy: 60.00648718780409\n",
            "Iteration: 9500. Loss: 0.10749445855617523. Accuracy: 60.42815439506974\n",
            "Iteration: 10000. Loss: 0.11004220694303513. Accuracy: 59.844307492701915\n",
            "Iteration: 10500. Loss: 0.1462155282497406. Accuracy: 60.07135906584496\n",
            "Iteration: 11000. Loss: 3.7984116077423096. Accuracy: 48.23224132338631\n",
            "Iteration: 11500. Loss: 0.2644876539707184. Accuracy: 58.96853713915018\n",
            "Iteration: 12000. Loss: 0.07488531619310379. Accuracy: 61.30392474862147\n",
            "Iteration: 12500. Loss: 0.020661216229200363. Accuracy: 60.979565358417126\n",
            "Iteration: 13000. Loss: 0.04814400523900986. Accuracy: 61.725591955887126\n",
            "Iteration: 13500. Loss: 0.016681203618645668. Accuracy: 60.94712941939669\n",
            "Iteration: 14000. Loss: 0.02401573210954666. Accuracy: 61.69315601686669\n",
            "Iteration: 14500. Loss: 0.009512044489383698. Accuracy: 60.16866688290626\n",
            "Iteration: 15000. Loss: 0.013792290352284908. Accuracy: 61.725591955887126\n",
            "Iteration: 15500. Loss: 0.10865630209445953. Accuracy: 61.20661693156017\n",
            "Iteration: 16000. Loss: 0.012585621327161789. Accuracy: 61.43366850470321\n",
            "Iteration: 16500. Loss: 0.007171025965362787. Accuracy: 61.10930911449886\n",
            "Iteration: 17000. Loss: 0.020826712250709534. Accuracy: 60.88225754135582\n",
            "Iteration: 17500. Loss: 0.0499909482896328. Accuracy: 60.265974699967565\n",
            "Iteration: 18000. Loss: 0.001981482608243823. Accuracy: 61.20661693156017\n",
            "Iteration: 18500. Loss: 0.001373359584249556. Accuracy: 60.55789815115148\n",
            "Iteration: 19000. Loss: 0.0009695675689727068. Accuracy: 61.466104443723644\n",
            "Iteration: 19500. Loss: 0.008739443495869637. Accuracy: 60.94712941939669\n",
            "Iteration: 20000. Loss: 0.0008158829296007752. Accuracy: 60.687641907233214\n",
            "Iteration: 20500. Loss: 0.00023854155733715743. Accuracy: 60.72007784625365\n",
            "Iteration: 21000. Loss: 0.00014583182928618044. Accuracy: 60.687641907233214\n",
            "Iteration: 21500. Loss: 9.71277549979277e-05. Accuracy: 60.655205968212776\n",
            "Iteration: 22000. Loss: 0.00014427918358705938. Accuracy: 60.72007784625365\n",
            "Iteration: 22500. Loss: 6.100715108914301e-05. Accuracy: 60.78494972429452\n",
            "Iteration: 23000. Loss: 8.646667265566066e-05. Accuracy: 60.78494972429452\n",
            "Iteration: 23500. Loss: 5.533122748602182e-05. Accuracy: 60.84982160233539\n",
            "Iteration: 24000. Loss: 3.0811432225164026e-05. Accuracy: 60.88225754135582\n",
            "Iteration: 24500. Loss: 5.94522753090132e-05. Accuracy: 60.88225754135582\n",
            "Iteration: 25000. Loss: 7.338248542509973e-05. Accuracy: 60.91469348037626\n",
            "Iteration: 25500. Loss: 4.564216214930639e-05. Accuracy: 60.91469348037626\n",
            "Iteration: 26000. Loss: 8.211444219341502e-05. Accuracy: 60.91469348037626\n",
            "Iteration: 26500. Loss: 7.84536823630333e-05. Accuracy: 60.94712941939669\n",
            "Iteration: 27000. Loss: 4.673808507504873e-05. Accuracy: 60.94712941939669\n",
            "Iteration: 27500. Loss: 5.563690501730889e-05. Accuracy: 60.979565358417126\n",
            "Iteration: 28000. Loss: 2.9492626708815806e-05. Accuracy: 61.012001297437564\n",
            "Iteration: 28500. Loss: 4.0678725781617686e-05. Accuracy: 60.94712941939669\n",
            "Iteration: 29000. Loss: 0.00010593038314254954. Accuracy: 60.94712941939669\n",
            "Iteration: 29500. Loss: 4.015989907202311e-05. Accuracy: 60.979565358417126\n",
            "Iteration: 30000. Loss: 5.135827814228833e-05. Accuracy: 60.94712941939669\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghmJGX38ReJN"
      },
      "source": [
        "# SETTINGS 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLk2QchkwkOR",
        "outputId": "b51dc05a-5934-43d5-c88c-d79ce3475775"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 200\n",
        "num_iters = 30000\n",
        "input_dim = 28*28 #num_features = 784\n",
        "num_hidden = 3000\n",
        "output_dim = 2\n",
        "\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)  \n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False) \n",
        "\n",
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.LeakyReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.LeakyReLU()\n",
        "\n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.LeakyReLU()\n",
        "\n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x.float())\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "        \n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = num_hidden)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images.float()) \n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 0.6900898218154907. Accuracy: 51.99481024975673\n",
            "Iteration: 1000. Loss: 0.688021183013916. Accuracy: 51.99481024975673\n",
            "Iteration: 1500. Loss: 0.6929113864898682. Accuracy: 52.18942588387934\n",
            "Iteration: 2000. Loss: 0.6917718648910522. Accuracy: 54.65455724943237\n",
            "Iteration: 2500. Loss: 0.6907588243484497. Accuracy: 55.98443074927019\n",
            "Iteration: 3000. Loss: 0.6911302804946899. Accuracy: 56.24391826143367\n",
            "Iteration: 3500. Loss: 0.6874823570251465. Accuracy: 56.762893285760626\n",
            "Iteration: 4000. Loss: 0.686323881149292. Accuracy: 57.73597145637366\n",
            "Iteration: 4500. Loss: 0.6927473545074463. Accuracy: 58.25494648070062\n",
            "Iteration: 5000. Loss: 0.6849923133850098. Accuracy: 58.384690236782355\n",
            "Iteration: 5500. Loss: 0.6825270056724548. Accuracy: 58.93610120012974\n",
            "Iteration: 6000. Loss: 0.6805939674377441. Accuracy: 58.64417774894583\n",
            "Iteration: 6500. Loss: 0.6829995512962341. Accuracy: 59.065844956211485\n",
            "Iteration: 7000. Loss: 0.6866599321365356. Accuracy: 59.4550762244567\n",
            "Iteration: 7500. Loss: 0.6806665658950806. Accuracy: 59.584819980538434\n",
            "Iteration: 8000. Loss: 0.6760271191596985. Accuracy: 59.552384041518\n",
            "Iteration: 8500. Loss: 0.6826258301734924. Accuracy: 59.39020434641583\n",
            "Iteration: 9000. Loss: 0.6786967515945435. Accuracy: 59.39020434641583\n",
            "Iteration: 9500. Loss: 0.6696600914001465. Accuracy: 59.29289652935452\n",
            "Iteration: 10000. Loss: 0.6875557899475098. Accuracy: 59.19558871229322\n",
            "Iteration: 10500. Loss: 0.6626766920089722. Accuracy: 59.130716834252354\n",
            "Iteration: 11000. Loss: 0.6859397888183594. Accuracy: 59.3577684073954\n",
            "Iteration: 11500. Loss: 0.6769599318504333. Accuracy: 59.519948102497565\n",
            "Iteration: 12000. Loss: 0.6874744892120361. Accuracy: 59.487512163477135\n",
            "Iteration: 12500. Loss: 0.6711687445640564. Accuracy: 59.552384041518\n",
            "Iteration: 13000. Loss: 0.6430650949478149. Accuracy: 59.61725591955887\n",
            "Iteration: 13500. Loss: 0.6743051409721375. Accuracy: 60.00648718780409\n",
            "Iteration: 14000. Loss: 0.6826804280281067. Accuracy: 60.16866688290626\n",
            "Iteration: 14500. Loss: 0.6756423711776733. Accuracy: 60.46059033409017\n",
            "Iteration: 15000. Loss: 0.6674731969833374. Accuracy: 60.52546221213104\n",
            "Iteration: 15500. Loss: 0.6891789436340332. Accuracy: 60.59033409017191\n",
            "Iteration: 16000. Loss: 0.6629152894020081. Accuracy: 60.33084657800843\n",
            "Iteration: 16500. Loss: 0.6754156351089478. Accuracy: 60.52546221213104\n",
            "Iteration: 17000. Loss: 0.6566066741943359. Accuracy: 60.42815439506974\n",
            "Iteration: 17500. Loss: 0.6538154482841492. Accuracy: 60.42815439506974\n",
            "Iteration: 18000. Loss: 0.6685289144515991. Accuracy: 60.72007784625365\n",
            "Iteration: 18500. Loss: 0.6648009419441223. Accuracy: 60.687641907233214\n",
            "Iteration: 19000. Loss: 0.6545500755310059. Accuracy: 60.88225754135582\n",
            "Iteration: 19500. Loss: 0.6604322195053101. Accuracy: 60.979565358417126\n",
            "Iteration: 20000. Loss: 0.6791099309921265. Accuracy: 60.78494972429452\n",
            "Iteration: 20500. Loss: 0.6577264666557312. Accuracy: 60.91469348037626\n",
            "Iteration: 21000. Loss: 0.6863650679588318. Accuracy: 61.044437236457995\n",
            "Iteration: 21500. Loss: 0.6584680676460266. Accuracy: 61.1417450535193\n",
            "Iteration: 22000. Loss: 0.6537851691246033. Accuracy: 60.979565358417126\n",
            "Iteration: 22500. Loss: 0.6611459851264954. Accuracy: 60.91469348037626\n",
            "Iteration: 23000. Loss: 0.6485739946365356. Accuracy: 60.91469348037626\n",
            "Iteration: 23500. Loss: 0.6651312112808228. Accuracy: 60.94712941939669\n",
            "Iteration: 24000. Loss: 0.6463323831558228. Accuracy: 60.52546221213104\n",
            "Iteration: 24500. Loss: 0.6700798869132996. Accuracy: 60.979565358417126\n",
            "Iteration: 25000. Loss: 0.6564645171165466. Accuracy: 60.91469348037626\n",
            "Iteration: 25500. Loss: 0.6752407550811768. Accuracy: 61.17418099253973\n",
            "Iteration: 26000. Loss: 0.6656311750411987. Accuracy: 61.17418099253973\n",
            "Iteration: 26500. Loss: 0.6498581767082214. Accuracy: 60.81738566331495\n",
            "Iteration: 27000. Loss: 0.6646491885185242. Accuracy: 60.979565358417126\n",
            "Iteration: 27500. Loss: 0.6641573905944824. Accuracy: 60.91469348037626\n",
            "Iteration: 28000. Loss: 0.6486024260520935. Accuracy: 60.88225754135582\n",
            "Iteration: 28500. Loss: 0.6498252153396606. Accuracy: 60.84982160233539\n",
            "Iteration: 29000. Loss: 0.6514892578125. Accuracy: 60.91469348037626\n",
            "Iteration: 29500. Loss: 0.6463905572891235. Accuracy: 61.17418099253973\n",
            "Iteration: 30000. Loss: 0.6506950855255127. Accuracy: 61.10930911449886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHh8T_dORbD_"
      },
      "source": [
        "# SETTINGS 3\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8EOmW0w7dPn",
        "outputId": "1f3fb61a-36f2-49a3-b7fd-8b4f11876552"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 200\n",
        "num_iters = 30000\n",
        "input_dim = 28*28 #num_features = 784\n",
        "num_hidden = 3000\n",
        "output_dim = 2\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)  \n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False) \n",
        "\n",
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.Tanh()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.Tanh()\n",
        "\n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.Tanh()\n",
        "\n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x.float())\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "        \n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = num_hidden)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images.float()) \n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 0.6600252389907837. Accuracy: 60.03892312682452\n",
            "Iteration: 1000. Loss: 0.6848621964454651. Accuracy: 60.42815439506974\n",
            "Iteration: 1500. Loss: 0.6641996502876282. Accuracy: 60.00648718780409\n",
            "Iteration: 2000. Loss: 0.669076144695282. Accuracy: 59.844307492701915\n",
            "Iteration: 2500. Loss: 0.6587255597114563. Accuracy: 60.13623094388583\n",
            "Iteration: 3000. Loss: 0.6496990323066711. Accuracy: 60.07135906584496\n",
            "Iteration: 3500. Loss: 0.6446557641029358. Accuracy: 59.74699967564061\n",
            "Iteration: 4000. Loss: 0.6756950616836548. Accuracy: 60.10379500486539\n",
            "Iteration: 4500. Loss: 0.6398617625236511. Accuracy: 59.61725591955887\n",
            "Iteration: 5000. Loss: 0.6508462429046631. Accuracy: 59.29289652935452\n",
            "Iteration: 5500. Loss: 0.6709608435630798. Accuracy: 60.265974699967565\n",
            "Iteration: 6000. Loss: 0.6728606224060059. Accuracy: 60.23353876094713\n",
            "Iteration: 6500. Loss: 0.6822946667671204. Accuracy: 58.384690236782355\n",
            "Iteration: 7000. Loss: 0.6450417041778564. Accuracy: 60.16866688290626\n",
            "Iteration: 7500. Loss: 0.6589035987854004. Accuracy: 59.909179370742784\n",
            "Iteration: 8000. Loss: 0.6240911483764648. Accuracy: 60.78494972429452\n",
            "Iteration: 8500. Loss: 0.6602491736412048. Accuracy: 60.298410638987995\n",
            "Iteration: 9000. Loss: 0.6477602124214172. Accuracy: 59.6496918585793\n",
            "Iteration: 9500. Loss: 0.6477848887443542. Accuracy: 60.42815439506974\n",
            "Iteration: 10000. Loss: 0.6733458638191223. Accuracy: 60.655205968212776\n",
            "Iteration: 10500. Loss: 0.6424055695533752. Accuracy: 60.00648718780409\n",
            "Iteration: 11000. Loss: 0.6372613310813904. Accuracy: 58.96853713915018\n",
            "Iteration: 11500. Loss: 0.6491756439208984. Accuracy: 57.89815115147584\n",
            "Iteration: 12000. Loss: 0.6196422576904297. Accuracy: 59.26046059033409\n",
            "Iteration: 12500. Loss: 0.644620418548584. Accuracy: 60.655205968212776\n",
            "Iteration: 13000. Loss: 0.6309717297554016. Accuracy: 61.07687317547843\n",
            "Iteration: 13500. Loss: 0.6189271211624146. Accuracy: 61.9850794680506\n",
            "Iteration: 14000. Loss: 0.6311626434326172. Accuracy: 60.201102821926696\n",
            "Iteration: 14500. Loss: 0.6691103577613831. Accuracy: 60.265974699967565\n",
            "Iteration: 15000. Loss: 0.656684935092926. Accuracy: 61.49854038274408\n",
            "Iteration: 15500. Loss: 0.6576241850852966. Accuracy: 61.20661693156017\n",
            "Iteration: 16000. Loss: 0.628567099571228. Accuracy: 60.72007784625365\n",
            "Iteration: 16500. Loss: 0.6232306957244873. Accuracy: 61.66072007784626\n",
            "Iteration: 17000. Loss: 0.6328312158584595. Accuracy: 60.46059033409017\n",
            "Iteration: 17500. Loss: 0.6426856517791748. Accuracy: 61.17418099253973\n",
            "Iteration: 18000. Loss: 0.6166785359382629. Accuracy: 62.179695102173206\n",
            "Iteration: 18500. Loss: 0.6315484046936035. Accuracy: 60.91469348037626\n",
            "Iteration: 19000. Loss: 0.5954779386520386. Accuracy: 61.49854038274408\n",
            "Iteration: 19500. Loss: 0.6548118591308594. Accuracy: 60.88225754135582\n",
            "Iteration: 20000. Loss: 0.623291552066803. Accuracy: 60.75251378527408\n",
            "Iteration: 20500. Loss: 0.6328985095024109. Accuracy: 62.04995134609147\n",
            "Iteration: 21000. Loss: 0.6412715315818787. Accuracy: 61.822899772948425\n",
            "Iteration: 21500. Loss: 0.6551916003227234. Accuracy: 61.401232565682776\n",
            "Iteration: 22000. Loss: 0.6470927596092224. Accuracy: 62.082387285111906\n",
            "Iteration: 22500. Loss: 0.6057913303375244. Accuracy: 58.709049626986705\n",
            "Iteration: 23000. Loss: 0.6275708675384521. Accuracy: 61.69315601686669\n",
            "Iteration: 23500. Loss: 0.6108640432357788. Accuracy: 62.04995134609147\n",
            "Iteration: 24000. Loss: 0.5981920957565308. Accuracy: 61.43366850470321\n",
            "Iteration: 24500. Loss: 0.622951328754425. Accuracy: 62.34187479727538\n",
            "Iteration: 25000. Loss: 0.6424969434738159. Accuracy: 60.84982160233539\n",
            "Iteration: 25500. Loss: 0.637157678604126. Accuracy: 61.466104443723644\n",
            "Iteration: 26000. Loss: 0.5781077742576599. Accuracy: 61.27148880960104\n",
            "Iteration: 26500. Loss: 0.6408781409263611. Accuracy: 60.23353876094713\n",
            "Iteration: 27000. Loss: 0.6303563117980957. Accuracy: 60.78494972429452\n",
            "Iteration: 27500. Loss: 0.5777852535247803. Accuracy: 61.53097632176451\n",
            "Iteration: 28000. Loss: 0.6003943681716919. Accuracy: 61.466104443723644\n",
            "Iteration: 28500. Loss: 0.6354464292526245. Accuracy: 59.6496918585793\n",
            "Iteration: 29000. Loss: 0.6079738140106201. Accuracy: 59.098280895231916\n",
            "Iteration: 29500. Loss: 0.5720544457435608. Accuracy: 61.33636068764191\n",
            "Iteration: 30000. Loss: 0.6095417737960815. Accuracy: 56.63314952967888\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28UQi5u8QmM_"
      },
      "source": [
        "# BEST SETTINGS THAT I'VE FOUND IN MY WORK = 62.24%\r\n",
        "\r\n",
        "BATCH_SIZE = 200 \\\\\r\n",
        "AT ITERATION 13500 \\\\\r\n",
        "USING 5 HIDDEN LAYERS (each having 300 nodes, and LeakyRELU as activation layer)\r\n",
        "Learining rate was 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUamEOatRS1o"
      },
      "source": [
        "# SETTINGS 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiZpqEXNPb51",
        "outputId": "3df0608c-1a29-440b-ae24-a581908c28bd"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 200\n",
        "num_iters = 13500\n",
        "input_dim = 28*28 #num_features = 784\n",
        "num_hidden = 300\n",
        "output_dim = 2\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)  \n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False) \n",
        "\n",
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.LeakyReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.LeakyReLU()\n",
        "\n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.LeakyReLU()\n",
        "\n",
        "        ### 4th hidden layer: 100 --> 100\n",
        "        self.linear_4 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_4 = nn.LeakyReLU()\n",
        "\n",
        "        ### 5th hidden layer: 100 --> 100\n",
        "        self.linear_5 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_5 = nn.LeakyReLU()\n",
        "\n",
        "        ### Output layer: 100 --> 2\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x.float())\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_4(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_4(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_5(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_5(out)\n",
        "        \n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = num_hidden)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images.float()) \n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 0.6913381814956665. Accuracy: 51.99481024975673\n",
            "Iteration: 1000. Loss: 0.6940190196037292. Accuracy: 51.99481024975673\n",
            "Iteration: 1500. Loss: 0.6906114220619202. Accuracy: 51.99481024975673\n",
            "Iteration: 2000. Loss: 0.6931421756744385. Accuracy: 51.99481024975673\n",
            "Iteration: 2500. Loss: 0.692850649356842. Accuracy: 52.12455400583847\n",
            "Iteration: 3000. Loss: 0.6921638250350952. Accuracy: 55.07622445669802\n",
            "Iteration: 3500. Loss: 0.6921141147613525. Accuracy: 55.20596821277976\n",
            "Iteration: 4000. Loss: 0.6891820430755615. Accuracy: 55.91955887122932\n",
            "Iteration: 4500. Loss: 0.6922417283058167. Accuracy: 57.41161206616932\n",
            "Iteration: 5000. Loss: 0.6895235180854797. Accuracy: 57.8657152124554\n",
            "Iteration: 5500. Loss: 0.6870647668838501. Accuracy: 58.22251054168018\n",
            "Iteration: 6000. Loss: 0.6856288313865662. Accuracy: 58.41712617580279\n",
            "Iteration: 6500. Loss: 0.6843570470809937. Accuracy: 59.065844956211485\n",
            "Iteration: 7000. Loss: 0.6782258749008179. Accuracy: 59.3577684073954\n",
            "Iteration: 7500. Loss: 0.6904815435409546. Accuracy: 59.39020434641583\n",
            "Iteration: 8000. Loss: 0.67177414894104. Accuracy: 59.97405124878365\n",
            "Iteration: 8500. Loss: 0.6668750643730164. Accuracy: 60.36328251702887\n",
            "Iteration: 9000. Loss: 0.6584930419921875. Accuracy: 61.044437236457995\n",
            "Iteration: 9500. Loss: 0.6557707190513611. Accuracy: 61.10930911449886\n",
            "Iteration: 10000. Loss: 0.6351891160011292. Accuracy: 61.30392474862147\n",
            "Iteration: 10500. Loss: 0.6532527208328247. Accuracy: 61.53097632176451\n",
            "Iteration: 11000. Loss: 0.675504744052887. Accuracy: 61.88777165098929\n",
            "Iteration: 11500. Loss: 0.6972792744636536. Accuracy: 62.21213104119364\n",
            "Iteration: 12000. Loss: 0.6374077200889587. Accuracy: 62.95815763866364\n",
            "Iteration: 12500. Loss: 0.6141052842140198. Accuracy: 62.21213104119364\n",
            "Iteration: 13000. Loss: 0.6348615884780884. Accuracy: 62.244566980214074\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEgHQOYgN8AX"
      },
      "source": [
        "'''\r\n",
        "SAVING THE MODEL\r\n",
        "'''\r\n",
        "if not os.path.exists(\"/content/saved_models/\"):\r\n",
        "    root_path=os.mkdir('/content/saved_models/')\r\n",
        "torch.save(model.state_dict(), '/content/saved_models/asm1_p2_60_16_acc.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}